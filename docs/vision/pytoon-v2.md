# Pytoon Video Generation System - Version 2 Master Specification

## Product Vision and Goals

**Evolution from V1 to V2:** Pytoon V1 generated videos by simple image stitching and basic pan/zoom effects, resulting in slideshow-like outputs. In V2, the vision is to transform this into true _cinematic composition_ - moving from the "animated GIF novelty" stage to professional-quality video generation[\[1\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=The%20AI%20video%20generation%20market,hard%20limits%20with%20existing%20solutions). Whereas V1 passively transitioned between static images, V2 introduces _real motion footage_, structured storytelling, and directorial control over scenes. This evolution enables **multi-shot narratives** instead of manual post-edit stitching[\[2\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=consistency%20Waiting%203%2B%20min%20for,Camera%20keyframes%20%26%20seed%20locking), meaning the system can craft a sequence of coherent scenes (shots) within one video.

**Real Motion and Directability:** A core goal of V2 is to produce **dynamic scenes with realistic motion** rather than static frames. Each scene can feature camera movement, subject movement, or visual effects akin to real video footage. The system prioritizes _directability_ - allowing creators (or an AI agent on their behalf) to explicitly orchestrate how the video unfolds. Users can specify or influence scene pacing, camera angles, and transitions, turning Pytoon into a "virtual director" rather than a simple assembler. This empowers _storytelling_: videos are composed as a series of scenes that convey a narrative or message, with intentional timing and flow, rather than an implicit sequence of assets. The system will parse high-level cinematic intentions (e.g. mood or style prompts) into a structured scene plan, similar to how advanced AI video tools use a _scene graph_ or shot list to maintain narrative coherence[\[3\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=The%20Cinematic%20Triangle%20Framework). By interpreting a user's creative prompts or script into a scene graph and timeline, Pytoon V2 ensures the final output feels like a directed mini-film rather than a random slide deck.

**Input Types and Target Output:** Pytoon V2 accepts _mixed media inputs_: - **Product images** - e.g. pictures of the product or brand assets that _must_ appear in the video (for authenticity and branding). - **Text prompts or scripts** - descriptions of scenes, desired visuals, or key marketing messages. These can include style/mood keywords (e.g. "upbeat futuristic scene in neon city") which the system will interpret into visual elements. - **Presets and brand guidelines** - reusable style presets (color schemes, fonts, logo overlays) or aspect ratio templates. For V2, the primary format is **short-form vertical video (9:16)** up to 60 seconds in length, as required by platforms like TikTok, Instagram Reels, and YouTube Shorts. The system ensures outputs are _platform-ready_, meaning correct aspect ratio, resolution (e.g. 1080×1920 for full HD vertical[\[4\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=You%20can%20generate%20directly%20for,and%20background%20consistency%20improved%20significantly)), and encoding for social media. By focusing on clips up to ~60s, Pytoon aligns with the sweet spot where AI video generation is currently most effective[\[5\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=turnaround%20stuff,publish%20all%20in%20one%20flow). The output must integrate seamlessly into social workflows (no additional cropping or editing needed) - a lesson learned from other tools now generating native 9:16 content to avoid "horizontal crop hell"[\[6\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=Google%20updated%20Veo%203,fit%20Instagram%20Reels%20or%20TikTok).

**Production Constraints:** Pytoon V2 prioritizes _brand safety and quality_ under real-world constraints. All content should adhere to brand guidelines (no distortions of logos, consistent use of approved colors/fonts, etc.). Videos must avoid explicit or disallowed visuals; the system will use safe prompts and filtering to prevent NSFW or off-brand outcomes. Length is capped at 60 seconds (with an ideal range of 15-45 seconds for most use cases). Each scene will typically be a few seconds (e.g. 3-10s) to keep viewers engaged in short-form context. The system optimizes for quick turnaround: generation of a full 60s video should ideally occur in under a couple of minutes of processing, leveraging efficient APIs and parallelization where possible. However, _quality will not be sacrificed for speed_ - the aim is **cinematic polish**: smooth transitions, synchronized audio, and cohesive visuals that meet professional marketing standards. In summary, Pytoon V2's goal is to let users turn a few images and a creative prompt into a polished, story-driven vertical video that is ready to publish, with minimal manual editing needed.

## Phase Map (Execution Order)

To implement Pytoon V2 in a controlled, autonomous manner, development is divided into five clear phases. Each phase has specific goals, allowed work, and strict exit criteria to ensure completeness before moving on. This phased roadmap guides an AI agent or engineering team through incremental builds with well-defined permission boundaries.

### PHASE-1: Architecture & Planning

**Goals:** Establish the core design and data structures of the system. Define the new abstractions (Scene Graph, Timeline Authority, Hybrid Engine Strategy) and how components will interact. Create a high-level plan for the pipeline and integration points for external engines and media.  
**Permitted Work:** In this phase, work is limited to design and prototyping. The agent can produce architecture diagrams, define JSON schemas, and write pseudo-code or interface definitions. Setting up project structure and stubs is allowed. However, no external API calls or heavy coding of features should occur yet - focus on planning.  
**Exit Criteria:** A complete specification of the system architecture is finalized and reviewed. This includes a documented Scene Graph schema, Timeline schema, and a component interaction diagram (e.g. Mermaid flowchart). All major modules (planner, timeline manager, engine interface, renderer, etc.) are identified with their responsibilities. The team/agent should have a test plan or example scenario drafted to validate the architecture. _Strict exit:_ The architecture document must cover how a sample input would flow to output using the new system, and all team members understand the plan. Only when the design is solid and no critical unknowns remain do we proceed.

### PHASE-2: Scene Sequencing MVP

**Goals:** Implement the basic scene composition pipeline on a timeline without real external video generation. This phase focuses on proving the timeline-based orchestration works using placeholder or simple content.  
**Permitted Work:** Coding the Timeline Authority and Scene Graph structures, and enabling multiple scenes to sequence together. The system should take a few example scenes (with dummy content like a solid color or a static image) and output a combined video timeline. Simple transitions (cuts or crossfades) between scenes can be implemented. Caption placement on the timeline can be prototyped with dummy text. **No integration with external AI video engines yet** - instead, use a stub (e.g., use the provided product image or a stock clip for each scene). Basic audio handling can start (e.g., concatenating an audio clip per scene if available, or silence).  
**Exit Criteria:** The system can produce a rudimentary multi-scene video from inputs using internal resources. For example, given 3 image files and 3 caption texts, it can create a 15-second video with those images in sequence, captions overlaid at the right times, and a transition effect between scenes. The timeline data (start/end times of each scene) matches the intended design exactly. _Strict exit:_ The output video's structure (scene order and durations) must match an input specification or test script. We should be able to inspect a "timeline JSON" from the system and see that it reconstructs the video structure accurately (proving the timeline authority works). Only once sequential scene assembly is verified do we integrate external engines.

### PHASE-3: Integrate Real Motion Engines

**Goals:** Connect the system to external Video Generation APIs and implement the **Hybrid Engine Strategy** for rendering scenes with real motion. In this phase, at least one API-based engine (e.g. Runway Gen-2, Pika Labs, or Luma AI) is incorporated to turn prompts into video clips for each scene.  
**Permitted Work:** The agent may now utilize external calls to video generation services. This includes writing code to format prompts and product image inputs for the API, sending requests, handling responses (video files), and storing the results. The Hybrid Engine Strategy logic is implemented: e.g., choose the appropriate engine per scene based on scene requirements or engine strengths (Runway for longer cinematic shots, Pika for stylized effects, Luma for realistic physics[\[7\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=%2A%20Platform%20specialization%20beats%20one,first%20campaigns)[\[8\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=While%20Runway%20and%20Pika%20compete,backed%20outputs%20with%20naturalistic%20motion)). Fallback mechanisms start to be coded (if an engine call fails or returns disallowed content, catch the error). Also permitted is refining the scene graph to hold engine-specific parameters (like prompt text, style preset, or reference image links). Minimal visual post-processing can be done here (e.g., scaling or trimming the returned clips to fit the timeline).  
**Exit Criteria:** At least one end-to-end test of real motion generation passes: e.g., "Scene 2 uses an API to generate a 5-second clip of a city skyline at night". The system should incorporate that clip into the timeline with surrounding scenes. The video output now contains actual moving footage in that scene. The engine invocation meets functional requirements: correct prompt formatting, proper handling of API keys/credits, and the video content is successfully retrieved and placed. _Strict exit:_ The video output for a test scenario must show a noticeable motion scene where expected, and if the API generation fails, the system logs it and falls back to a static image or placeholder without crashing. Only after proving integration with one engine (and perhaps multiple engine options) do we proceed.

### PHASE-4: Audio, Voice & Caption Synchronization

**Goals:** Introduce the audio layer - voice-over narration and synchronized captions tied to the timeline. Ensure that narration (or generated voice) aligns with the visual scenes, and on-screen text captions appear in sync with speech.  
**Permitted Work:** Implementation of voice-over handling is enabled. This could involve taking a provided voice recording or using a text-to-speech (TTS) engine to generate a narration from a script or the prompt. The system should align the voice audio with the timeline (e.g., split or time the narration so that each sentence or phrase lands in the correct scene). Using speech-to-text or transcript alignment techniques (like forced alignment) is allowed to map words to timestamps. The Captioning subsystem is built: it will generate subtitle captions from the transcript and place them on the timeline within each scene's duration. The system should support **caption sequencing** - for instance, if the voiceover has two sentences corresponding to Scene 1 and Scene 2, each caption should appear only during its respective scene interval. Caption styling (font, size, position according to presets) can be implemented here. The audio pipeline must handle _audio ducking_: adding background music under the voice and automatically lowering the music volume when the voiceover is speaking[\[9\]](https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA#:~:text=Using%20Audio%20Ducking%20to%20Balance,speech%20can%20be%20clearly). Permitted work includes using audio libraries to mix tracks and ensure voice clarity.  
**Exit Criteria:** A full video with voiceover and captions passes testing. For a given input script, the system produces a voice track (or uses the provided one) and places captions such that **each caption aligns with the spoken words** in time. The synchronization should be tight - within ~0.1 second accuracy between audio and caption[\[10\]](https://arxiv.org/pdf/2512.18318#:~:text=,ment%2C%20achieving%20an). If the narrator says "Our product is amazing" in scene 1 from 0s to 4s, then the caption text "Our product is amazing" is visible only during that 0-4s interval (with maybe slight lead/trail). The background music (if any) audibly ducks during the speech and comes back up in pauses, confirming the ducking logic. _Strict exit:_ When playing the output, observers should note that voice narration matches the scene visuals and caption text without delay; any subtitle or audio misalignment beyond ~100ms is considered a failure to fix before proceeding. The video should now be cohesive with visual, voice, and text elements synchronized.

### PHASE-5: Refinement, Multi-Engine Support & Reliability

**Goals:** Finalize all features, optimize for reliability, and ensure the system meets all V2 specifications. This includes supporting multiple engine options, robust fallback behaviors, and polishing the output quality (transitions, overlays, etc.).  
**Permitted Work:** This phase allows any remaining feature development needed for full V2 functionality. The Hybrid Engine Strategy is expanded - the system can intelligently select among different video generation APIs or combine them. For example, one scene might use Runway for a realistic shot while the next uses Pika for a creative effect, based on predefined rules or scene tags. All **brand-safe overlays and constraints** are implemented now: e.g., adding a watermark or logo overlay if required, enforcing that no scene violates brand safety (blurring or replacing any problematic AI-generated frames). Transitions between scenes can be upgraded beyond simple cuts (e.g., implement fade-through-black, swipe, or other effects as allowed by presets). The audio mixing is refined for polish - ensure no pops, smooth fade-in/out of music, consistent volume levels. Performance improvements (caching engine outputs, parallelizing scene generation) are done if needed to meet efficiency goals. Crucially, **reliability logic** is solidified: comprehensive error handling and fallback for each potential failure point (engine errors, missing assets, etc.). Testing and QA automation (including running the Acceptance Criteria checklist) is performed in this phase, and any issues are fixed.  
**Exit Criteria:** Pytoon V2 meets all defined acceptance criteria (see the index below) in a test suite of scenarios. The system has demonstrated that it _always produces a usable video even if some subsystems fail_. For instance, if an API engine is unavailable, the video still renders using fallback imagery - no hard errors are shown to the user. Multi-engine functionality is verified by configuring different engines for different scenes and seeing the expected results (and if one engine returns poor output, trying an alternate automatically). _Strict exit:_ The final system is considered **feature-complete and stable** when it passes 100% of acceptance tests, including edge cases (network failure, bad prompt input, etc.), and has been reviewed by stakeholders (engineering, QA, product) for sign-off. Only at that point is Version 2 considered ready for deployment.

## System Architecture Overview

Version 2 introduces several new core abstractions and a modular architecture to support cinematic video generation. The major components and their interactions are outlined here, along with the key data structures (_Scene Graph_ and _Timeline_) and the **Hybrid Engine Strategy** that blends AI generation with deterministic rendering.

### Core Abstractions and Data Structures

- **Scene Graph:** In Pytoon V2, a _Scene Graph_ is the structured representation of each scene's content and how scenes connect. It's essentially a hierarchical model of the video's narrative elements. Each node in the scene graph can represent an element like a background setting, a product image, text overlay, or effect, along with relationships (which object is in which scene, what order). This abstraction allows fine-grained control of scene composition - similar to how new AI video tools map out elements, lighting, camera angles, and motion in a graph tied to prompts[\[11\]](https://key-g.com/pt/blog/veo-3-the-future-of-video-generation-now-with-visual-instructions/#:~:text=Adopt%20a%20modular%20prompt%20schema,and%20precise%20updates%20across%20frames). For example, a scene graph might specify: Scene 1 contains _Product Image A_ at center, with _Caption Text_ at bottom, under _Lighting: warm_, _Camera: slow zoom_. Scene 2 might reuse _Product Image A_ but in a different context (size or position), plus a _Background Video Clip_ generated via prompt. The scene graph encapsulates this so that the system knows exactly what each scene consists of and can maintain consistency (e.g., carry the same product image into multiple scenes). It is also used by the AI planning module - e.g., a language model could output a scene graph structure from a prompt (listing scenes and their content), providing deterministic cinematography tokens rather than leaving it implicit[\[12\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=Successful%20Seedance%20prompting%20follows%3A%20Cinematic,Scene%20Graph%20%E2%86%92%20Camera%20Tokens)[\[13\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=,camera%20tokens%20for%20deterministic%20cinematography). In implementation, the Scene Graph could be a JSON document or Python object graph describing all scenes and their assets (see schema example below).
- **Timeline Authority:** The Timeline Authority (or Timeline Orchestrator) is the component responsible for turning the scene graph into a timed sequence - it governs _when_ each scene and media element plays in the final video. We dub it an "Authority" because it centrally coordinates timing, ensuring no overlaps unless intended and enforcing the overall video length and pacing. This module takes the scenes and assigns start times, durations, and transitions. It is aware of frame rates and timecodes. If the Scene Graph is like the screenplay and set pieces, the Timeline Authority is the director scheduling each scene on the shooting timeline. It aligns events to the timeline, e.g., "Scene 1 from 0s-5s, then a 1s crossfade, Scene 2 from 6s-10s," etc. When interactive or conditional events are needed (not common in linear videos, but hypothetically triggers like camera moves or effects at certain times), the timeline is where they are executed[\[14\]](https://key-g.com/pt/blog/veo-3-the-future-of-video-generation-now-with-visual-instructions/#:~:text=To%20support%20%D0%B8%D0%B3%D1%80%D1%8B%20scenarios%2C%20define,clear%20prompts%20rather%20than%20guesswork). This authoritative separation means even if generation engines produce variable-length clips, the timeline module will trim or extend scenes as needed to fit the intended structure. All caption timings and audio tracks are also managed here - essentially, nothing appears in the video without a defined timeframe. The Timeline data is typically stored in a JSON structure listing each scene's start/end and any transition, plus tracks for audio and captions.
- **Hybrid Engine Strategy:** Rather than relying on a single generative model to output the entire video (which could lead to "hallucinated" random clips or inconsistencies), Pytoon V2 employs a **hybrid template engine approach**[\[15\]](https://glama.ai/mcp/servers/@naki0227/auto-cm-director#:~:text=The%20Hybrid%20Architecture). This strategy splits creative responsibilities between an AI and a deterministic renderer. The _AI Engine(s)_ are used for what they do best - generating visuals (or short video segments) from prompts - acting as creative content generators. Meanwhile, a _template/assembly engine_ (the Timeline & Scene Graph orchestration with a video compositor) ensures those pieces are compiled correctly every time. In other words, AI is the "Creative Director" and the rendering engine is the reliable "Editor." By blending multiple engines, the system can mitigate the weaknesses of any single model. For example, if Runway's model excels at realistic human motion and Luma's at physics for products[\[8\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=While%20Runway%20and%20Pika%20compete,backed%20outputs%20with%20naturalistic%20motion), the system can use each for appropriate scenes. If an engine fails or yields poor quality, the system falls back to simpler methods or alternative engines without breaking the entire pipeline - an essential design for graceful degradation[\[16\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,just%20error%20messages)[\[17\]](https://qodex.ai/blog/openai-sora-api#:~:text=3,to%20simpler%20functionality%20when%20needed). The Hybrid Engine Strategy also refers to mixing traditional video editing techniques with AI: for instance, using a stable library like FFmpeg or Remotion to do the final compositing ensures a "perfect compile" of the video even if AI content varies[\[18\]](https://glama.ai/mcp/servers/@naki0227/auto-cm-director#:~:text=Unlike%20traditional%20%22Text,uses%20a%20Hybrid%20Template%20Engine). This architecture acknowledges that "one-size-fits-all" AI is not practical; specialized engines (Runway, Pika, Luma, etc.) will be orchestrated together to achieve the best result for each scene[\[7\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=%2A%20Platform%20specialization%20beats%20one,first%20campaigns). The system's Engine Manager component (discussed below) encapsulates this strategy.

### Major Components and Their Interactions

The Pytoon V2 system is composed of modular components, each with a distinct role, communicating as follows:

- **Input Handler:** The entry point that ingests user inputs - e.g., uploading product image files, entering the text prompt or script, selecting a style preset. It validates inputs (correct file formats, acceptable prompt language) and then passes them to the Planner.
- **Scene Planner:** This module (sometimes called the "Director AI") interprets the user's creative intent and generates the initial Scene Graph. It may involve an AI step: for example, using an LLM to parse a textual description into a sequence of scenes with details. The planner decides _how many scenes_, what happens in each, and which inputs go to which scene. It attaches the appropriate style preset or cinematic style to each scene as needed. For instance, if the prompt says "Show the product then demonstrate it in use," the planner might output a Scene Graph with Scene1: static product beauty shot, Scene2: action shot of product being used with an AI-generated background. This component ensures the video has a logical flow and adheres to storytelling goals. The output is the structured **Scene Graph** data passed along.
- **Engine Manager (Multi-Engine Orchestrator):** This component receives the scene specifications and decides which _content generation engine_ to use per scene. It implements the **Hybrid Engine Strategy** by interfacing with external APIs (Runway, Pika, Luma, etc.) as well as internal rendering options. For each scene, it checks the desired content:
- If the scene calls for **real motion video** from a prompt, the Engine Manager chooses the best API. For example, for a scene needing photorealism and smooth camera work, use Runway's Gen model; for a highly stylized quick effect, use Pika's engine; for a product shot with realistic physics (like a spinning 3D view), use Luma[\[8\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=While%20Runway%20and%20Pika%20compete,backed%20outputs%20with%20naturalistic%20motion). These decisions can be rule-based (e.g., flags in scene description) or learned from presets.
- If the scene is simple (e.g., just overlaying a provided image or doing a Ken Burns pan on an image), the Engine Manager may decide _no external AI needed_ - instead use an internal method or template.
- The Engine Manager handles API communication: sending the prompt, images, and style parameters to the engine and retrieving the resulting video clip. It is also responsible for engine _fallbacks_. If one engine fails (times out or returns an error/content violation), it can automatically try a backup engine or switch to a non-AI method. For instance, if the Pika API fails to return a result, the Engine Manager might call Runway with a simpler prompt, or as last resort use a static image with a zoom effect so the timeline isn't broken. This design ensures there is _no single point of failure_: the video generation continues even if some AI calls fail, aligning with industry best practices of making AI optional to the core experience[\[19\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,capable%20with%20progressive%20enhancement).
- **Timeline & Composition Builder:** This is the heart of the **Timeline Authority**. It takes the Scene Graph (with references to the media assets each scene will use, whether generated or static) and arranges them on the timeline. This component knows the total video duration target and each scene's intended length (the planner or engines might suggest durations). It ensures scenes are concatenated correctly and inserts transitions between them. For example, if Scene1 is 4.0 seconds and Scene2 is 3.5 seconds, it might insert a 0.5s crossfade or a hard cut at the 4.0s mark. The composition builder uses a deterministic approach (akin to a video editing software's timeline) to layer visuals and audio. It will layer the **captions** onto each scene (from the Caption Manager) and also plan out how the audio tracks (voice and music) stretch across the timeline. By the time this module finishes, there is effectively a complete "edit decision list" for the video: all video clips with start/end times, which caption appears when, and where audio fades in/out.
- **Audio & Caption Manager:** A specialized component that deals with the spoken and written content. It either accepts a user-provided voiceover or generates one via Text-to-Speech. It then splits or aligns this narration to scenes - e.g., by using punctuation or semantic breaks to map sentence 1 to Scene1, sentence 2 to Scene2, etc. It produces a **caption track** by transcribing the voice (or using the known script) and timing each caption to the audio (using forced alignment so that words are highlighted at the right times). This manager also loads or generates background music, if a music track is part of the preset. Crucially, it performs **audio ducking** on the timeline: marking segments where voiceover is present and ensuring the music volume is lowered in those segments[\[9\]](https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA#:~:text=Using%20Audio%20Ducking%20to%20Balance,speech%20can%20be%20clearly) (and raised during voice pauses). The output of this component is synchronized audio tracks ready to merge and a caption timeline that the Composition Builder will overlay visually.
- **Video Renderer/Compositor:** Finally, the rendering engine (which could be built on a library like MoviePy, FFMPEG, or a framework like Remotion) takes the timeline with all assets and renders the final video file. It fetches each video clip or image asset for each scene (some may be in memory from the Engine Manager, others like product images from storage), applies the specified transitions (e.g., using FFMPEG filters for crossfade), overlays the captions text at the set times (burned-in subtitles or separate subtitle track), and mixes the audio tracks (voice and music) with the calculated volume levels. The result is encoded as an MP4 (or similar) in the required vertical resolution. The Renderer is kept **dumb but robust** - it should always be able to compile the video given the timeline, even if the content is simple. This ensures reliability: by separating creative generation from assembly, we guarantee that as long as we have _some_ media for each scene (even a placeholder), the final rendering will not crash. The Renderer thus acts as the safety net (e.g., it can fill missing segments with black frames or a default card if needed rather than failing).

Below is a simplified **data flow** diagram illustrating how these components interact from input to output:

flowchart LR  
subgraph User_Inputs  
A1(Product Images)  
A2(Text Prompt / Script)  
A3(Style Presets & Constraints)  
end  
subgraph "Pytoon V2 System"  
direction TB  
B\[Scene Planner (Director AI)\]  
C\[\[Scene Graph Data\]\]  
D\[Timeline Authority & Composition Builder\]  
E\[Engine Manager (Hybrid Engines)\]  
F\[Audio & Caption Manager\]  
G\[Video Renderer / Editor\]  
end  
A1 & A2 & A3 --> B  
B --> C  
C --> D  
C -.-> E:::async  
C -.-> F  
E -- Generated Clips --> D  
F -- Voice, Music, Captions --> D  
D --> G  
G --> H{Video Output}  
H --> I\[Final 9:16 MP4 Video\]  
<br/>classDef async color:#245;

_Figure: High-level data flow through Pytoon V2._ The Scene Planner creates a Scene Graph which is used by the Timeline/Composition module. In parallel, the Engine Manager may generate clips for scenes (asynchronously), and the Audio/Caption Manager generates the audio tracks and captions. The Timeline Authority then composes everything and passes it to the Renderer for final output.

### Component Interaction Details

To further clarify the component relationships, consider how a sample video is generated: 1. **Scene Planning:** User provides 2 product images and a prompt "Show product, then demonstrate it in action in a fun way." The Scene Planner creates Scene Graph with two scenes: Scene1 (product image focus, static), Scene2 (product image with AI-generated dynamic background of a crowd using it). 2. **Engine Invocation:** Engine Manager sees Scene2 needs an AI video. It chooses an engine (e.g., Pika for creative effect) and sends the prompt plus product image as input. Meanwhile, Scene1 is just an image - no engine call needed. 3. **Timeline Construction:** Timeline Authority schedules Scene1 for 0-5s and Scene2 for 5-10s, adding a crossfade transition at 5s. 4. **Audio/Caption:** The user provided no voiceover, so the system uses TTS on the prompt or script. Suppose it generates one sentence per scene (Voice: "Introducing XYZ", then "Experience it in action"). Audio Manager aligns sentence1 to 0-5s, sentence2 to 5-10s. Background music is loaded and will duck during those sentences. Captions for each sentence are prepared ("Introducing XYZ" for scene1, etc.). 5. **Rendering:** Once Pika returns the video clip for Scene2, the Renderer layers Scene1 image (with maybe a slow zoom effect to simulate motion) with caption text "Introducing XYZ", then crossfades into the Pika video clip for Scene2, overlays the second caption text, and mixes in the voiceover and music (music volume lowered during the voiceover parts). The final result is encoded.

All components are decoupled to some extent, communicating via the defined data contracts (Scene Graph JSON, Timeline, etc.). This modular design makes it easier to swap engines or adjust timing without affecting unrelated parts. For instance, if a new engine "Runway Gen-5" becomes available, the Engine Manager can be extended to call it for certain scenes without changing how the timeline or audio is managed.

**Diagrams & Illustrations:** Below is a schematic component architecture diagram, showing major subsystems inside Pytoon V2 and their connections:

flowchart LR  
subgraph "Pytoon V2 Architecture"  
direction LR  
P1\[Input Handler\] --> P2\[Scene Planner\]  
P2 --> P3\[Scene Graph Store\]  
P3 --> P4\[Timeline Orchestrator\]  
P4 --> P5\[Video Composer\]  
P3 --> P6\[Engine Manager\]  
P3 --> P7\[Caption & Audio Manager\]  
P6 --> P5  
P7 --> P5  
end  
P5 --> P8\[Output Video File\]

_Figure: Component architecture._ The **Scene Graph Store** is a central data model accessible to the Engine Manager and Caption/Audio Manager. The **Timeline Orchestrator** and **Video Composer** together represent the Timeline Authority, assembling visual layers and audio on a fixed schedule. The Engine Manager and Caption/Audio Manager enrich the scene data with media content and timing before final composition.

## Developer Specification and Requirements

This section details the functional requirements and technical specifications developers must follow to implement Pytoon V2. It covers scene segmentation logic, engine usage, timeline captioning, rendering of motion scenes via APIs, assembly of video/audio, and ensuring brand safety. Schema examples for the key data structures (Scene Graph and Timeline) are included to illustrate expected formats.

### Scene Planning and Segmentation

- **Dynamic Scene Segmentation:** The system shall convert user input (prompt/script) into a sequence of scenes that form a logical narrative. Each scene is a coherent unit (like a shot in film) focusing on a particular message or visual. The planner should use punctuation or semantic breaks in the prompt to decide scene boundaries, or default to ~5-10 second scenes if unspecified. For example, a 30s video might be automatically segmented into ~5 scenes of a few seconds each, unless the user directs otherwise.
- **Scene Graph Structure:** Each scene in the internal representation must include metadata about its content: e.g., a unique scene ID, intended duration, references to media (which product image or which AI prompt to use), style tags (mood, camera motions), and the text (caption or voice script) associated with that scene. This forms the Scene Graph JSON. The scene graph can be hierarchical if needed (e.g., a scene node containing child nodes for overlay elements like text or images).
- **Continuity and Reuse:** The planner should allow "carry-over" of elements between scenes when appropriate for continuity[\[2\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=consistency%20Waiting%203%2B%20min%20for,Camera%20keyframes%20%26%20seed%20locking). For instance, if the same product appears in multiple scenes, it should be represented by the same asset reference in the graph to encourage consistency (the Engine Manager can then attempt to maintain consistency across scenes, e.g., a character or product looks the same in each scene).
- **Directability via Prompt:** The system must interpret special directives in the prompt for scene planning. If a user specifies multiple shots or uses a screenplay-like format (e.g., "&lt;SHOT 1&gt; … &lt;SHOT 2&gt; …"), the planner should respect those as explicit scene definitions. This draws from patterns where creators specify multiple shots to get multi-scene output[\[12\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=Successful%20Seedance%20prompting%20follows%3A%20Cinematic,Scene%20Graph%20%E2%86%92%20Camera%20Tokens). The planner must parse these cues into separate Scene Graph entries, including any provided camera or style hints.
- **Timing Estimation:** Each scene node in the graph should have an estimated duration initially. If the voiceover is driving the timing, use the length of that voice segment; if not, default durations can be assigned (with a total cap of 60s). The Timeline Authority will adjust these as needed, but the plan should sum up roughly to the target video length.
- **Example Scene Graph Schema:** (for a two-scene video)

{  
"scenes": \[  
{  
"id": 1,  
"description": "Product hero shot",  
"duration": 5000, /\* in milliseconds \*/  
"media": {  
"type": "image",  
"asset": "product1.png",  
"effect": "ken_burns_zoom" /\* pan/zoom on static image \*/  
},  
"caption": "Introducing the XYZ Gadget",  
"audio": null /\* no voice specific to this scene, will use global voice segment \*/  
},  
{  
"id": 2,  
"description": "Product in action, user scenario",  
"duration": 8000,  
"media": {  
"type": "video",  
"engine": "runway",  
"prompt": "A person using the XYZ Gadget outdoors, cinematic lighting",  
"asset": null /\* will be filled with generated clip path \*/  
},  
"caption": "Experience innovation in every frame",  
"audio": null  
}  
\],  
"globalAudio": {  
"voiceScript": "Introducing the XYZ Gadget. Experience innovation in every frame.",  
"backgroundMusic": "uplifting_track.mp3"  
}  
}

_Schema Note:_ This JSON illustrates how scenes might be defined. Scene 1 uses a product image with a Ken Burns effect, Scene 2 requires a video from the Runway engine with a given prompt. Each has a caption text. A global voice script is provided which likely will be split across scenes (by the audio manager).

### Engine Invocation and Fallback

- **Integration with External Engines:** The system must support invoking external video generation APIs via HTTP calls or SDKs. This includes formatting requests with proper authentication and handling the asynchronous nature if needed (polling for result, etc.). Supported engines in V2 should include at minimum: **Runway Gen-2 (or Gen-4)**, **Pika Labs**, and **Luma AI** (or equivalents), as these cover a range of strengths (Runway for high-quality cinematics, Luma for physics-realism, Pika for creative effects[\[7\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=%2A%20Platform%20specialization%20beats%20one,first%20campaigns)[\[8\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=While%20Runway%20and%20Pika%20compete,backed%20outputs%20with%20naturalistic%20motion)). The Engine Manager should abstract these so that adding/removing an engine is easy (e.g., via plugin or configuration).
- **Engine Selection Strategy:** Implement rules for choosing an engine per scene. For example: "If scene description contains 'realistic' or requires precise camera control, use Runway; if scene contains 'stylized' or quick social-media style, use Pika; if scene mentions product physics or 3D, use Luma." These rules can be refined over time or even user-selectable. The system might also allow the user to force a particular engine via a preset (e.g., a user could choose "use Luma for all scenes" if they prefer).
- **Prompt Construction:** The Engine Manager should construct prompts for the APIs that leverage the scene information. This may include combining the user's prompt snippet for that scene with automatic context (like adding camera instructions or style keywords derived from presets). For example, if the scene style is "neon-noir" and it's a city scene, the actual prompt sent to the engine might append those style cues. This draws on known prompting frameworks (e.g., Cinematic Intention → Scene Graph → Camera Tokens as used in multi-shot prompting[\[3\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=The%20Cinematic%20Triangle%20Framework)). Ensure that critical details (like the product image, if the engine supports image input) are attached so the generated content stays on-message.
- **Asynchronous Handling:** Many video generation APIs are not instant. The system should call an engine and either receive a video URL/bytes or an operation ID to poll. It must handle waiting for the result without stalling the entire pipeline. Possibly, engine requests can be done in parallel for multiple scenes to speed up generation. The architecture should treat engine outputs as _future promises_ that the Timeline Composer will wait on or fill in once ready.
- **Fallback and Graceful Degradation:** For each engine call, implement robust error handling. If an engine fails (HTTP error, or returns content that violates policy or is low-quality/unusable), the system should:
- Attempt an alternate engine with a simplified prompt (if time permits) - e.g., if Runway fails, try Pika or vice versa.
- If alternatives fail or are not available, use a deterministic fallback: for example, use the product image or a stock video clip to fill the scene duration. The fallback content should be something neutral (perhaps a simple animated background with the product image) so that the video can still continue. **No scene should simply be omitted** without replacement, to avoid a gap.  
    This approach ensures the system _never leaves the user empty-handed_, aligning with the principle that AI features can fail without breaking core functionality[\[19\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,capable%20with%20progressive%20enhancement). In effect, the core product - producing a video - should succeed 100% of the time, even if it's a simpler version when AI fails. The user might be notified that "some scenes used fallback visuals due to an engine issue" but they will still get a complete video.
- **Engine Response Validation:** After receiving a video clip from an engine, the system should quickly validate it. Validation includes: correct format and length (if a 5s clip was expected but a 1s or 30s came back, flag it), content safety (run checks if the content might contain disallowed visuals or severe artifacts), and relevance (if possible, e.g., using a captioning or CLIP analysis to ensure the video roughly matches the prompt). If validation fails, trigger the fallback path as above.
- **Resource Management:** Using external engines might incur costs or rate limits. The Engine Manager must ensure it uses them efficiently - e.g., avoid requesting two similar videos if one could be reused, and cancel any pending requests if they're no longer needed (like if a scene is dropped). If multiple scenes use the same engine with similar prompts, consider batching or reusing results to optimize. Also, ensure proper API key security (keys stored safely, not exposed).
- **Logging and Debug Info:** For maintainability, every engine invocation should be logged with scene ID, engine used, prompt, and result (success/fail and latency). This helps in debugging issues and improving the strategy (for example, if Pika consistently returns poor results for certain prompts, developers can adjust rules or prompt templates).

### Timeline-Based Captioning

- **Automatic Caption Generation:** The system shall generate captions (subtitles) for all voice/spoken content in the video. If the user provides a script or the text prompt serves as the narration, that text is the basis for captions. If a voiceover file is provided without text, the system should run speech-to-text transcription to get the content for captions (using a reasonably accurate ASR).
- **Synchronization with Voice:** Captions must be synchronized with the voiceover audio track to within ±100 milliseconds[\[10\]](https://arxiv.org/pdf/2512.18318#:~:text=,ment%2C%20achieving%20an). This means implementing forced alignment: splitting the transcript by words or phrases and aligning them to the audio's timing. Tools or libraries can be used (e.g., Gentle or AWS Transcribe with word timestamps). At minimum, each caption line (usually a sentence or phrase) should start no later than when that speech actually starts, and end when speech ends. Ideally, if the voice is long, mid-sentence captions can break at natural pauses to avoid too much text on screen.
- **Caption Timing tied to Scenes:** Importantly, caption segments should be attached to the scenes in which they are spoken. For instance, if Scene 2 covers 5s-10s and the voiceover sentence starts at 5.2s and ends at 7s, that caption is part of Scene 2 and should not overflow into Scene 3. The Timeline Authority should enforce that captions do not carry over scene boundaries (unless a deliberate stylistic choice like a continuous voiceover bridging scenes, but in such cases captions still usually break per scene for readability). Each scene can have one or more caption events in the timeline JSON, and these need to fall between the scene's start and end times.
- **Caption Styling:** The system should apply a consistent style to captions, customizable via presets. For example, brand font and color, semi-transparent background behind text if needed, position (typically bottom center for social videos). If multiple languages or any special formatting (like highlighting a keyword) are needed, those would be extensions; basic V2 scope is one language caption matching the narration.
- **Editing and Accuracy:** The text of captions should exactly match the spoken words (unless minor edits are needed for brevity - some systems shorten captions slightly for readability, but since our script is presumably short, we prefer exact match). If the prompt or TTS didn't produce a perfect line, developers might allow a "caption override" input for users to fix wording. For QA, each caption must be spell-checked and reviewed for grammar. The acceptance criteria will include that captions _must_ correspond correctly to speech and scenes (no missing or extra captions).
- **Multi-Modal Alignment (Optional):** If a scene has no voiceover (e.g., just background music), but there is important text to show (like a slogan), the system can treat that text as a "caption" for display purposes. Essentially, even silent scenes can have on-screen text, which should still be represented in the timeline so that it appears for the correct duration.
- **Accessibility Consideration:** The captioning should be robust enough that if someone watches the video muted, they can still follow the message via the captions. This is a common scenario on social platforms. Therefore, ensure captions are always visible with enough contrast and are synced to convey the story without audio.

### Real Motion Scene Rendering via API Engines

- **API Support:** The system must interact with external Video Generation APIs to render scenes that require motion. This involves sending requests to services like RunwayML, Pika Labs, and Luma AI. Each of these services has its own API interface; the system should implement wrappers or clients for each. For example, Runway's Gen-2 might require a JSON payload with prompt and desired length; Pika might take an image + prompt; Luma's "Dream" engine might focus on a prompt for cinematic output. The developer must refer to each API's documentation for request/response format and implement accordingly.
- **Video Clip Specifications:** When requesting generation, specify the needed clip properties to the API if possible: e.g., resolution (1080x1920 vertical), duration (in seconds or frames), and any style guidance. If an API allows setting aspect ratio or resolution, always request 9:16 vertical format to avoid cropping after (some latest models support native vertical generation[\[6\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=Google%20updated%20Veo%203,fit%20Instagram%20Reels%20or%20TikTok)). If an API cannot do 9:16, the system should handle cropping or padding in post. Duration control is important - some engines might not precisely hit the duration, but the system can either request a few seconds more and trim or use whatever is returned and let the timeline cut it to fit.
- **Asset Injection:** For scenes that involve the user's product images or other assets, the system should feed those into the generation where possible. e.g., if the engine supports "init image" or "reference image", use the product shot to guide the scene. Some engines might allow uploading the image and then referencing it in the prompt or as part of conditioning. If not, an alternative is to generate a background video and later composite the actual product image into it (though that could look less natural). The ideal: find engines or methods to include the product in the generation for realism. (Luma's approach of realistic rendering might be useful if 3D models are involved, but that's beyond V2 scope unless product is simple.)
- **Quality Consistency:** Real motion scenes from different engines must still feel like parts of one video. Developers should mitigate differences in color or clarity by post-processing if needed (e.g., apply a color filter across all scenes for uniform look, or add a slight grain to all content). The scene graph could carry a "global style" that the compositor applies to each clip to make them cohesive. This ensures that switching engines (Runway vs Pika) mid-video isn't jarring to viewers.
- **Engine Quota and Latency:** Because these API calls can be slow (several seconds to minutes) and may have usage quotas, the implementation should include:
- Progress updates if possible (especially if integrated into an agent workflow - the agent might need to know to wait). Possibly a spinner or log message "Generating Scene 2 via Pika…".
- Timeouts: if an engine takes too long (e.g., > some threshold like 60 seconds), abort and fallback as mentioned. We don't want the whole video creation to hang indefinitely.
- Respect rate limits: space out calls or queue them if necessary. If the user requests many videos in parallel, you may need a throttling mechanism.
- **Error Handling:** If an API returns an error or undesired result, capture the error details for debugging. E.g., if the content filter rejects the prompt (common if something triggers it erroneously[\[20\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=attempts%20,just%20error%20messages)), log that and possibly modify the prompt automatically (remove or replace certain words) and retry once. This is an advanced feature but can improve reliability (for example, if "shoot" in a prompt triggers a filter, replace with "film").
- **Output Integration:** Once a clip is obtained, it should be stored (temporarily) in the project workspace (e.g., as an MP4 file or a series of frames). The Timeline Composer will then treat it like any other video asset. The engine output must be converted if necessary to a standard format - e.g., if an API returns a GIF or a silent MP4, ensure we can use it (adding silent audio track if needed so FFMPEG doesn't cut it, etc.). The system should also trim or extend the clip to the exact duration needed: e.g., if the engine returned 6 seconds but we only want 5 seconds, cut off the extra or speed-adjust slightly if minor.
- **Resource Cleanup:** After final rendering, any heavy intermediate files (like raw video from engines) can be cleaned from temp storage to save space, unless caching for potential reuse. If caching, tag them by prompt hash so we don't regenerate identical scenes within some timeframe (this can be an optimization if multiple similar videos are made).

### Directed Assembly of Video, Caption, Audio, and Transitions

- **Explicit Timeline Assembly:** Unlike V1, which implicitly assembled content, V2 must _explicitly_ assemble according to the directed plan. The video editing pipeline should essentially reproduce what a human editor would do given the same storyboard. That means the implementation should treat the timeline as law: each scene's start/end, each transition type, and each asset placement must be honored as in the timeline data. If any adjustments are made (e.g., a scene's length changes due to engine output), the timeline data structure should be updated accordingly so that the source of truth is always accurate.
- **Video Transitions:** Support at least basic transition types: cut (instant switch), crossfade (overlap dissolve), and possibly fade-through-black. Transition choice can be default or based on preset (e.g., a "playful" theme might use a playful wipe transition). Implement these using the chosen video library: for example, FFMPEG filters can do crossfades between clips, or in Remotion one can overlap two scene components with opacity. The transition duration (typically 0.5s - 1s) should be configurable. Ensure that transitions do not distort content (maintain aspect ratio, etc.).
- **Layered Video Composition:** Many scenes might involve layers - e.g., the background video and an overlaid product image or text. The assembly module should support layering multiple tracks. For instance, a scene might call for a generated background clip _plus_ the product image superimposed (perhaps with transparency) _plus_ a caption text overlay. The system needs to composite these: i.e., place the background, resize/position the product image on top (maybe sliding it in or animating it), then render the text caption on top of all. This is akin to having multiple video tracks in an editor. Using a library like MoviePy or an HTML/Canvas approach (Remotion uses React) can simplify layering. The requirement is that all specified overlays and animations in the scene description are realized in the output.
- **Caption Rendering:** Captions should be rendered as text overlays (unless we choose to output a separate subtitle file, but most likely we burn-in for platform readiness). The rendering should account for safe areas (e.g., not too low to be cut off by mobile UI - typically keep subtitles a bit above the bottom). If using FFMPEG, this could be done via the drawtext filter; if using Remotion/Canvas, just by text elements. The caption text and timing come from the Caption Manager. Style (font, color, background) comes from preset. Test on various lengths of text to ensure no clipping (maybe implement auto line-wrapping if a caption is long - 2 lines max ideally).
- **Audio Layering and Mixing:** The assembly should mix down possibly multiple audio tracks into the final audio. At least two tracks: voiceover and background music. There could also be sound effects per scene (not explicitly in spec, but possibly small things like a "click" sound on a transition if needed). All audio must be mixed to a single output track (stereo). Implement volume adjustments: the background music track should have a base volume (maybe -15dB relative to voice) and duck further (e.g. to -30dB) when voice is active[\[9\]](https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA#:~:text=Using%20Audio%20Ducking%20to%20Balance,speech%20can%20be%20clearly). The voiceover track should remain clear and near 0dB (peaking around -3dB ideally). If scenes butt together, ensure no audio pops - maybe apply small fade-out at scene end and fade-in next scene's audio if they are disjoint. However, since likely we have one continuous voiceover, that might stretch across scenes; in that case, keep it as one track and don't chop it per scene to avoid discontinuity. Align music so that it covers the whole video length (loop or trim as needed) and ends gracefully (fade out at end of video). Developers should use an audio library or FFMPEG audio filters for this mixing.
- **Frame Rate and Encoding:** The final assembly should use a consistent frame rate (e.g., 30fps) unless specified otherwise. Combine all visuals accordingly. For encoding, use H.264 codec for MP4, with a bitrate that gives good quality for 1080x1920 (e.g., ~5-8 Mbps for 30s video). Ensure keyframe intervals that align with smooth playback (not crucial but for completeness). The output should be playable on common devices and meet platform requirements (for example, many social platforms require < 500MB file, which is trivial for 60s of 1080p at that bitrate).
- **Preview and Iteration:** The design of directed assembly also allows re-editing: developers should keep the pipeline flexible so that if changes are needed (e.g., user wants scene 2 shorter), it's easy to regenerate just the timeline and re-render without regenerating everything from scratch. This might mean caching engine outputs or at least being able to reuse them if only timing changed. It's more of an optimization, but the code structure should separate "generation" steps from "composition" steps clearly.
- **Timeline JSON Schema:** A JSON structure for the final timeline might look like:

{  
"timeline": \[  
{ "sceneId": 1, "start": 0, "end": 5000, "transition": "cut" },  
{ "sceneId": 2, "start": 5000, "end": 13000, "transition": "fade" }  
\],  
"tracks": {  
"video": \[  
{ "sceneId": 1, "asset": "product1.png", "effect": "zoom", "layer": 0 },  
{ "sceneId": 2, "asset": "scene2_runway.mp4", "layer": 0 },  
{ "sceneId": 2, "asset": "product1.png", "layer": 1, "transform": {"position":"center","scale":0.5} }  
\],  
"audio": \[  
{ "type": "voiceover", "file": "voice.mp3", "start": 0 },  
{ "type": "music", "file": "music.mp3", "start": 0, "volume": 0.5,  
"duckRegions": \[ {"start":0,"end":4000}, {"start":5000,"end":8000} \] }  
\],  
"captions": \[  
{ "text": "Introducing the XYZ Gadget", "start": 500, "end": 4000 },  
{ "text": "Experience innovation in every frame", "start": 5500, "end": 7500 }  
\]  
}  
}

This timeline JSON shows two scenes (IDs 1 and 2) with their time spans and transitions. It lists video tracks: Scene1 uses an image with a zoom effect; Scene2 uses a generated clip plus an overlay image (product) on layer 1. Audio tracks include a voiceover starting at 0 and a music track that plays throughout at 50% volume with ducking during 0-4s and 5-8s (when voice is speaking). Captions are timed within their scenes. The renderer would use this data to compile the final video.

### Audio Layering, Ducking, and Sync to Voice

- **Multi-Track Audio Support:** The system must handle at least two simultaneous audio tracks - typically voice and music. It should be designed to potentially handle more (like sound effects or multiple voices) by extension, but V2 scope is voice + background music. All tracks need to be mixed down to a final stereo output.
- **Voice-over Integration:** If a voiceover file is provided, the system should use it as-is (perhaps trimming silence at start/end). If no voiceover, the system uses the TTS generated audio. In either case, treat this as the primary audio track. The voice track will often dictate the timing of scenes (if narration goes scene by scene). Ensure the voice track's sample rate is compatible with the output (resample to a standard like 44.1 kHz or 48 kHz if needed). No pitch or tempo alterations should happen to voice unless explicitly intended (to preserve natural sound).
- **Background Music:** The system should allow an optional background music track that plays under the entire video or certain portions. The developer should provide a way to select a music track (perhaps from a library or user upload) or have default theme music for presets. The music should be edited to fit the video length - common approaches: cut to length with a fade-out, or loop seamlessly if it's shorter than the video. For a more polished result, some music tracks might have editable segments (intro, loop, outro) that can be algorithmically fitted, but that's optional.
- **Ducking Implementation:** Implement ducking so that whenever voice (or important foreground audio) is present, the background music volume is lowered to avoid clashing. One straightforward method: detect the segments where voice is active (we have those timestamps from caption alignment or simply the voice clip's waveform). For those segments, reduce the music volume by a certain ratio (e.g., -12 dB which is about 25% volume). The reference from Adobe indicates tools exist to do this automatically[\[21\]](https://helpx.adobe.com/premiere/desktop/add-audio-effects/adjust-volume-and-levels/automatically-duck-audio.html#:~:text=Automatically%20duck%20audio%20,dialogue%20or%20voiceover%20is) - we need to implement similarly in code. The "duckRegions" in the timeline example above indicates where to lower volume. The fade-out/fade-in of music around those boundaries should be smooth (apply a small transition, e.g., 0.2s fade). The goal is for voice to be clearly audible without needing to manually adjust by the user[\[9\]](https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA#:~:text=Using%20Audio%20Ducking%20to%20Balance,speech%20can%20be%20clearly).
- **Audio Sync and Latency:** End-to-end latency (voice vs video) should be handled so that there is no noticeable lip-sync issue - since we are likely not animating lips but showing captions, the key is caption sync which we covered (±100ms). Ensure the voice audio starts exactly at the time it should relative to the first scene. For example, if we want a half-second of silence before speaking, that can be built in; otherwise voice starts at scene start. If voice audio files have any offset (sometimes recordings have silence), handle that (trim or offset in timeline).
- **Volume Normalization:** Ensure the combined audio mix is normalized to avoid clipping. The final mix should probably peak around 0 dB (with maybe a limiter to catch any overs). We could use an automatic gain control or just manual setting (voice at natural level, music at background level). The spec could say: voice average level should be around -6 dBFS, music around -18 dBFS when under voice (and maybe -12 dBFS when solo). This ensures voices are about 10-15 dB louder than music when overlapping, consistent with broadcast standards.
- **Multiple Voices or Dialogues:** Not explicitly in scope, but if there were multiple narrators or dialogues, the system would need to orchestrate that (assign speakers to scenes, etc.). For now, assume one continuous narration for simplicity.
- **Muting and Failures:** If either audio track is missing (e.g., no voice and user doesn't want TTS, or no music), the system should still output correctly (just one track audible). If voice is missing, perhaps raise music a bit since nothing to duck. Also, if an audio generation fails (like TTS service down), fallback could be to just use on-screen text and music - but that should be an exception as voice is key.
- **Waveform Considerations:** For future enhancements, analyzing the waveform of music to align scene cuts to beats would be nice (to make transitions hit on strong beats), but that's a stretch goal. Not required in V2, but keep architecture open for possibly reading beat timestamps and adjusting scene timings by a few frames to align with music (it can greatly enhance perceived quality).

### Brand-Safe Overlays and Constraints

- **Logo and Watermark Overlay:** Many marketing videos require a brand logo to be shown persistently or at least at the end. The system should allow adding a **brand watermark/logo** overlay. This could be a small semi-transparent logo in a corner throughout, or a full-screen logo at the end (like an outro). The requirement is that the developer can easily include such an overlay via the preset or an input parameter. Technically, it means supporting image overlay layers in the composition. For example, if a preset says "Apply logo.png at top-left throughout", the composition should add that as a layer on all scenes (or on the timeline track spanning the whole duration). Ensure the logo image is scaled appropriately for the video resolution and doesn't cover important content or captions.
- **Safe Visual Zones:** Adhere to platform safe zones (e.g., on TikTok/Reels, the bottom and top areas often have UI elements like captions and buttons). While not strictly a development requirement, it's a design constraint: e.g., avoid placing critical text or logos in the bottom 150px or so and top 100px of a 1080x1920 frame. If needed, presets can define these margins, and the caption placement should consider them.
- **Content Moderation:** The system should enforce brand safety by avoiding or altering any AI-generated content that might be inappropriate. Developers should integrate some form of content filtering or at least prompt sanitization. For example, before sending a user prompt to an engine, remove or flag prohibited words. Also consider the engine's own filters - handle cases where an engine rejects a prompt for possibly containing sensitive content[\[20\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=attempts%20,just%20error%20messages). In such cases, either modify the prompt or fallback to a safe scene. If a generated video somehow includes unexpected elements (like the engine drew a competitor's logo or a random text), the system should detect and mask it if possible (this is complex, but maybe have a quick scan for any textual regions in frames and blur them if not from our captions). At minimum, the team should manually review outputs during development to catch obvious issues.
- **Length and Format Compliance:** Brand or platform guidelines might specify max length (we have 60s), certain intro/outro requirements, etc. The system should have a configuration to enforce these. E.g., if a brand requires a 2-second logo animation at end, that could be injected as a final scene by default. Or if a platform has a 60s limit, ensure the timeline never goes beyond 60000ms. If the user input would make it longer, either truncate or prompt the user to shorten (maybe phase 1 planning can handle that by compressing scenes).
- **Color and Font Consistency:** Presets can include brand colors and fonts for text. The caption text and any additional graphics should use these to stay on-brand. For example, if the brand font is _Open Sans_, ensure the caption renderer uses it (which means the font file should be available to the rendering engine). Colors for text or background elements should default to brand's palette. If an AI engine is generating backgrounds, we might not fully control colors, but perhaps we could steer prompts slightly (e.g., "with brand colors of red/white"). That might be too advanced; likely, just ensure overlays (text, shapes) match brand colors.
- **No Competitor Mentions:** If relevant, ensure the prompt doesn't accidentally bring in a competitor's name or if the user prompt does that, consider warning them or replacing it. (This might not be a big concern unless the AI has a tendency to put random text in images, which some do. E.g., stable diffusion sometimes produces gibberish text - which could accidentally look like a word; our post-processing could detect text via OCR and remove it if present, but that's advanced).
- **Testing Brand Safety:** QA should include checking the final video for any flashes of unintended content. For instance, ensure that between scene transitions no single frame has something odd (video diffusion can sometimes have flicker of unwanted frames). The deterministic nature of our timeline should mitigate that by controlling crossfades. If found, the system might need to regenerate or adjust the seed for that engine (some engines allow specifying a seed for deterministic output, which could be a feature: if we want reproducibility or slight variations, we might expose seed locking as advanced parameter[\[22\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=L20%20No%20multi,Camera%20keyframes%20%26%20seed%20locking) - but that's optional).
- **User Constraints:** If the user or admin sets constraints (like "do not use more than X seconds of AI footage" or "do not show the product upside-down"), the system should attempt to respect these through logic or prompt shaping. These are domain-specific but worth noting that the architecture allows such rules to be added without overhauling it.

In summary, the developer must build the system so that it _always adheres to brand and platform guidelines automatically_, reducing the need for manual video editing after generation. The output should be ready to publish unless the user chooses to tweak it further.

## Acceptance Criteria Index (Updated for V2)

The following is a checklist of testable acceptance criteria for Pytoon V2. These criteria must be met for the system to be considered fully functional and ready for release. They cover synchronization, correctness of assembly, and fallback behavior, and are designed to be specific enough for QA testers or automated agents to verify (some could even be automatically checked by an AI agent or script).

- \[ \] **Scene Structure Matches Plan:** The number of scenes in the output video matches the intended number from the input or scene plan. If the user requested N scenes or the prompt implies N segments, the final video has N distinct scenes (verify via scene transition points in the video). No extra unplanned scenes are present.
- \[ \] **Captions Align to Scenes:** Each scene's caption text is displayed only during that scene's duration and not beyond. When a scene transition occurs, the previous scene's caption disappears and the next scene's caption (if any) appears. Captions do not overlap multiple scenes unintentionally.
- \[ \] **Voice-Text Synchronization:** If voiceover narration is used, the on-screen captions are synchronized to the voice within ±100 milliseconds[\[10\]](https://arxiv.org/pdf/2512.18318#:~:text=,ment%2C%20achieving%20an). There should be no noticeable delay between spoken words and highlighted caption text. This can be tested by checking several timestamps in the output: the moment a word is spoken vs. the caption timeline metadata.
- \[ \] **Accurate Caption Content:** Captions exactly match the narration or provided script (barring minor approved edits). Every spoken word that is intelligible should appear in text, and no captions display text that isn't spoken. Spelling and grammar in captions are correct and reviewed.
- \[ \] **Scene Timing Consistency:** The start and end times of each scene in the final rendered video correspond to the timeline data. For example, if the timeline JSON says Scene2 starts at 5.0s and ends at 10.0s, the rendered video indeed shows the transition around 5s and 10s marks (with minor variance of a couple of frames tolerance). Essentially, the video reflects the planned timeline exactly.
- \[ \] **No Visual Glitches at Transitions:** Scene transitions are smooth as per the specified effect (cut or fade, etc.). There are no single-frame glitches, flashes, or jumps at the cut points. Especially check crossfades: both scenes should properly overlap for the fade duration with correct opacity blend.
- \[ \] **Voiceover Completeness:** The voiceover audio spans the intended parts of the video. It should neither cut off early nor extend past the video. If the script has two sentences for two scenes, both sentences are heard fully in their respective scenes. Silence in voiceover (pauses) should only be where intended (e.g., slight pause between sentences/scenes) and not due to an error.
- \[ \] **Background Music Ducking:** During voiceover sections, the background music volume is lowered appropriately so that the voice is clearly audible[\[9\]](https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA#:~:text=Using%20Audio%20Ducking%20to%20Balance,speech%20can%20be%20clearly). This can be checked by waveform analysis or listening: voice should dominate, music audible but not overpowering. In sections with no voice, music returns to normal volume. The transitions in volume should be smooth (no abrupt jumps in volume).
- \[ \] **Audio Sync and Quality:** All audio tracks (voice and music) are in sync with the video and each other. There are no noticeable audio delays or early/late audio. No pops or clicks between scenes. The overall audio volume is normalized - no clipping or extreme quietness. (e.g., final loudness could be around -14 LUFS for consistent social media loudness, though we won't enforce exact metric here).
- \[ \] **Engine Content Incorporated:** For any scene that was supposed to use an AI generation (Runway/Pika/Luma), confirm that the final video _does_ include motion content in that scene (not just a blank or a static image unless fallback triggered). For example, if Scene3 was defined to use an engine, the output in Scene3 should show a moving video background or effect as expected. If engine completely failed and fallback static content was used, mark this test as conditionally pass if fallback is indeed present and the scene isn't missing. Essentially: no scene is left empty - either the AI content is there or the fallback content is present.
- \[ \] **Fallback Graceful Degradation:** Induce a failure (e.g., disable network to engine or use a prompt that triggers a content filter) and verify the system still produces a complete video. In the output video under failure conditions, all scenes are present (maybe with simpler visuals) and the video does not crash or abort. The system should possibly log or indicate that fallback was used, but to the viewer, the video is still coherent (even if less fancy). This tests the rule that optional AI can fail without breaking core flows[\[19\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,capable%20with%20progressive%20enhancement).
- \[ \] **Consistent Style & Branding:** The visual style is consistent across scenes. Colors and lighting are reasonably coherent (no one scene drastically out-of-place unless intended). If a brand preset was used, verify that the brand's font is used in captions, the brand colors appear where expected (e.g., caption background or text in correct color), and any logo watermark is correctly placed and persists (if it should). For example, if the preset says logo at top-right, check that it's visible throughout and not cut off.
- \[ \] **Resolution and Aspect Ratio:** The output video file has resolution 1080x1920 (or the configured vertical resolution) and aspect ratio 9:16 exactly, with no black bars added. Check the file metadata or play it on a phone to confirm it's truly vertical full-screen.
- \[ \] **Duration <= 60s:** The total runtime of the video does not exceed 60 seconds (unless explicitly configured to allow longer for some reason). If user asked for shorter (e.g., 30s), ensure it's within a second of that target (some slight variance might occur from scene rounding or transitions, but should be minimal).
- \[ \] **File Format Compatibility:** The output is a standard MP4 (H.264/AAC) that plays on common devices and platforms. Test by opening the file in a typical player and uploading to a test social media account to ensure no format issues.
- \[ \] **No Critical Content Errors:** The video content should reflect the user's request without introducing errors. For instance, if the prompt was about a "sneaker product", the video should indeed show something relevant (like people running or the sneaker), not some unrelated concept. This is a somewhat subjective check, but essentially verify that the AI didn't hallucinate something entirely off (within reason of current AI limits). If something obviously wrong appears (like text "Lorem ipsum" artifact or a competitor's logo), that's a fail.
- \[ \] **Captions Legibility:** Captions are sized and colored for legibility on a smartphone screen. Test by viewing on a mobile-sized display: text should be readable (not too small), and contrast with background should be sufficient (use outlines or semi-opaque background if necessary). No caption text extends off screen or gets cut off.
- \[ \] **Engine Efficiency (Optional/Stretch):** The system meets a basic performance benchmark: e.g., a 30s video with one AI scene and voice can be generated in under X minutes (the threshold can be something like 2 minutes). This is more of a performance acceptance than functional; it ensures the pipeline is reasonably optimized (we won't strictly fail the build for performance in V2, but we set expectations).
- \[ \] **Logging and Debug Info (for QA):** While not part of the user-facing product, ensure that during generation, the system outputs logs or has a debug mode where one can trace what happened (which engine was used for which scene, any fallbacks triggered, timing info). This is important for QA and future troubleshooting. For acceptance, a tester could run a generation with debug on and confirm that key steps are logged (e.g., "Scene2: Requested Pika API… received result in 15s" etc.). This criterion ensures transparency of the process during testing.

Each of these criteria can be systematically checked. Many can be verified by inspecting the timeline JSON or logs against the output video (for an agent, it could parse the video or trust the internal representation). The system should ideally output or save the final Scene Graph and Timeline used, to assist in verification. The expectation is **100% of these must pass** (unless a particular feature is not in use for a test, in which case it's N/A). The criteria marked with references highlight the importance of those points (e.g., synchronization and graceful degradation).

## Rendering Flow and Orchestration

In Pytoon V2, the rendering pipeline is an orchestrated sequence of stages that convert user inputs into the final video. This section describes the full flow, highlighting where key decisions and sequencing occur. The emphasis is on how we move from an implicit, automated assembly (V1 approach) to an _explicit, directed orchestration_ in V2.

### End-to-End Pipeline Steps

- **Input Ingestion:** The user (or calling program) provides inputs: one or more product images, a text prompt or script, and any preset selection. The system validates these (correct file types, prompt not empty, etc.).
- **Scene Planning (Director Phase):** Using the prompt and any structure it contains, the system breaks the content into an ordered list of scenes. This involves either using simple heuristics (split by sentences) or an AI planner. _Decision point:_ The system decides how many scenes and what each scene's role is (intro, demo, outro, etc.). It also decides on approximate durations here and if any particular images map to specific scenes. For example, it might decide "Scene1 will showcase product image A for 4 seconds with text, Scene2 will be an AI-generated context scene for 6 seconds," etc. This is the first stage where the system's _explicit direction_ comes in: it's creating an actual plan rather than just throwing all assets into a timeline arbitrarily.
- **Scene Graph Construction:** From the plan, a Scene Graph data structure is built. This includes populating each scene node with known details (like attach product image A to Scene1). If the prompt included style or camera directions (say the user wrote "Slow pan of the city" for a scene), those details are attached to that scene's metadata. The scene graph now explicitly outlines what must happen in each scene. (In V1, by contrast, no such structured representation existed; V1 likely just had a list of images to show in order, without rich metadata).
- **Timeline Initialization:** The Timeline Authority creates a timeline structure, laying out scenes in order with their intended start/end based on durations. At this point, the timeline is like a skeleton - it knows scene slots and lengths, and reserved gaps for transitions. _Decision point:_ Here the system may adjust timing if the sum of scene durations isn't 100% certain (e.g., if total is under 60s, might add slight padding somewhere; if over, trim down durations proportionally or as decided). The transitions are also decided: e.g., set all to crossfade default or whatever the preset dictates. This is an explicit decision-making moment: nothing is left implicit about how scenes connect - the timeline data is authoritative.
- **Audio/Voiceover Processing:** In parallel, or next, the Audio Manager processes narration. If a script is available (from user prompt or provided text), and no recorded voice is provided, a TTS service is called to generate the voice audio. If a voice file is provided, it's transcribed (if we need text for captions). _Decision point:_ The system decides how to map the narration to scenes. Often it's sequential (first sentence = Scene1, etc.), but if one sentence is very long, perhaps it spans two scenes or requires breaking. The audio manager explicitly sets which timestamp each sentence should end by so it fits the scene length (e.g., if Scene1 is 4s, ideally the first sentence audio should end by ~4s). If the voiceover is longer than scene allocation, the system might extend the scene or consider adjusting the timeline (this is a potential iterative feedback: timeline might need tweaking to fit voice - explicit coordination is needed here).
- **Caption Generation:** Using the final transcript from the voiceover, captions are segmented and timed. The system explicitly times each caption to the voice waveform (ensuring alignment within the scene as discussed). It then places these caption events into the timeline under their respective scenes. This is an area of **explicit sequencing**: in V1, captions might have been auto-generated but likely not precisely timed to scenes; in V2, we intentionally sequence caption events on the timeline.
- **AI Engine Invocation:** For each scene marked as needing AI generation (like a video clip from a prompt), the Engine Manager now takes over those scenes. It sends requests to the chosen engines with the scene's prompt and parameters. This step can happen asynchronously: multiple scenes could be generating in parallel if resources allow. The orchestration here is that the timeline has placeholders, and the system _knows_ it must fill them. It's not leaving it to chance - the engine results will be mapped back to specific scene slots. _Decision point:_ If an engine returns and something is off (too long, etc.), the system decides how to handle it. For example, if the returned video is 8s but scene was planned for 6s, either trim 2s off or consider extending the timeline if there's slack. Because V2's approach is explicit, it leans towards trimming to fit the plan (unless the content is critical, we usually won't extend the total video beyond target). If trimming, the system might choose which part of clip to use (likely start from 0 for simplicity or pick best segment). These micro-decisions are made by rules (e.g., always cut off extra at end unless it would cut mid-word of voice).
- **Fallback Orchestration:** If any AI generation fails at this stage, the system immediately triggers the fallback path for that scene. For instance, if by a certain time no result is returned, it logs a warning and uses the backup content (maybe just uses the product image with some filter as the scene background). Importantly, it doesn't change the overall timeline; it simply populates the scene with an alternate asset. The orchestration ensures continuity: even with fallback, the scene time slot is filled. (In V1, a failure might have just resulted in a missing segment or the system might not have had a concept of fallback at all).
- **Media Assembly Prep:** Once all scenes have their media assets ready (either from engine or fallback or just original images), the Composition Builder now finalizes the timeline for rendering. It assembles a list of all media sources in order, with proper timing, and all overlays. _Decision point:_ The system explicitly orders overlay rendering here. For example, if a product image should appear on top of a generated background in Scene2, it will ensure the product PNG is layered above the video and perhaps schedules a simple animation if needed (explicit positioning and scaling over time can be part of scene data). Transitions between scenes are also prepared (perhaps by duplicating a few frames or crossfade segments). Essentially, the timeline is now fully populated with real asset references and ready to render.
- **Rendering/Encoding:** The final step is handing off this composed timeline to the rendering engine. If using FFMPEG, this means generating the filter complex and commands to do the overlays, concatenations, and audio mix. If using a higher-level tool (like Remotion or MoviePy), it means feeding the timeline data into that environment to synthesize the video. This step is largely mechanical execution of the plan: no new decisions happen here except any on-the-fly adjustment needed by the codec (like if the last scene audio is shorter, it might pad a fraction of a second of silence to mux properly, etc.). The output is then saved as a video file.

Throughout these steps, **decision-making is made explicit** in the data structures: The Scene Graph explicitly encodes what content goes where; the Timeline explicitly encodes when things happen. This is a shift from any implicit timing or ordering. For instance, in V1 if you added images, maybe it just equally spaced them or something without an explicit plan. In V2, every timing is computed and can be reviewed or modified before rendering. This explicit direction allows for much finer control and easier debugging (one can adjust the JSON and re-render if needed).

### Transition from Implicit to Explicit Direction

In V1, much of the assembly might have been implicit or fixed: e.g., images in order of input, each on screen for X seconds, maybe automatically fit to the audio length without clear per-scene control. There was likely no concept of a _scene_ beyond perhaps each image being a segment. Audio might have just played over the slideshow without tight sync, and captions might have been auto-scrolled without scene context.

**In V2, the assembly is explicitly directed** in these ways:

- **Defined Scenes:** We formalize the notion of scenes with boundaries, rather than a nebulous sequence. This means the system (and user indirectly) can reason about the video in chapters or sections. It's no longer a black-box that merges everything; it's more like a script that it follows.
- **Deterministic Timeline:** Every cut and transition is planned. If the system decides a crossfade, that's recorded. The exact moment one scene ends and the next begins is set in stone in the timeline data. Implicit assembly (like automatically spacing content) is replaced by deterministic scheduling. This yields predictability: running the same inputs through should produce the same timing every time (especially because we can even fix random seeds for AI engines if needed for consistency[\[22\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=L20%20No%20multi,Camera%20keyframes%20%26%20seed%20locking)).
- **Alignment of Modalities:** With explicit orchestration, the video, audio, and text are all aligned by design. For example, because we explicitly tie sentence 1 to scene 1, we won't have a case where the voice talks about scene2 content while scene1 visuals are still playing - which could happen in a less directed system if things drifted. Each modality (visual, textual, auditory) is synced by referencing the same timeline clock.
- **Traceability:** Each output element can be traced back to the plan. If a caption appears at 5s, we know from the timeline which scene and script line that is from. If an image appears, we know which scene node it came from. This makes testing and adjusting easier (we can pinpoint which scene to tweak if something looks off). Implicit assembly would not have this clarity; explicit direction gives us that audit trail.
- **Extensibility:** Because the assembly is explicit, adding new features becomes easier. Want to add a new type of transition or an overlay effect? We can do so by extending the scene schema and teaching the renderer, rather than relying on a one-size implicit algorithm. Similarly, we could allow user input to influence the timeline ("make scene2 longer") because we have a concrete handle on it.

To illustrate the orchestration, consider that we have essentially implemented a mini video-editing workflow programmatically. Pytoon V2's pipeline can be likened to a human editor using a timeline: importing clips (from AI or library), cutting them to length, placing them in order, adding text overlays and audio tracks in sync. The difference is the human decisions are replaced by AI and rule-based decisions, but made explicit in data. This approach gives us both **automation and control**.

One more scenario: suppose the user doesn't provide a prompt, just images and says "make a video". The system can still operate - perhaps it uses a default storyline template from presets (explicitly defined scenes like "Intro - feature highlights - closing"). It shows that explicit orchestration can even drive content creation from minimal input by relying on templates and default decisions, rather than doing something vague. Those templates would essentially be pre-defined scene graphs and timeline patterns that the system can fill with the user's images and some generic text. This is much more powerful and predictable than a V1 system that might just throw images with default transitions without context.

In summary, the rendering flow carefully coordinates every step. Decision points such as scene split, engine choice, and timing adjustments are handled in dedicated modules, making the process transparent. The final orchestration yields a video where nothing is left to coincidence: each element is there by plan, fulfilling the vision of turning user's input into a directed, cinematic sequence.

## Reliability and Fallback Behavior

Pytoon V2 is designed with reliability in mind, ensuring that even if certain advanced features or external services fail, the system will still produce a complete and usable video. This section defines the rules and mechanisms for **graceful degradation** and fault tolerance throughout the video generation process.

### Graceful Degradation Principles

The overarching principle is that **core functionality should not depend on any single fragile component**[\[16\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,just%20error%20messages). In practice, this means the video gets made one way or another, every time. Optional enhancements (AI-generated motion, fancy effects, etc.) should improve quality when available, but their absence should not break the workflow[\[19\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,capable%20with%20progressive%20enhancement). The system defaults to simpler methods if needed, rather than showing errors to the user.

### Fallback Scenarios and Rules

- **AI Engine Failure:** If an external video generation API fails (due to network, timeout, or content moderation rejection), the system will:
- Log the failure event with details (which engine, error message or code).
- Immediately attempt an alternate engine if configured for that scene. For example, if Runway fails, try the prompt on Pika or vice versa, as long as the scene type is compatible.
- If alternates also fail or none available, fallback to a static visual for that scene. The static visual could be:
  - The product image on a plain or lightly animated background (e.g., a zoom/pan effect on the product image itself, so at least something is moving).
  - Or a previously generated frame or placeholder (like a "scene unavailable - using placeholder" slide, though better to use the product or some relevant graphic to not confuse the viewer).
- The timeline for that scene remains the same length; we just replace the content. This ensures the subsequent scenes and audio stay aligned.
- The system should also reduce dependency on that engine in future if failures are frequent (this is more for system tuning, but not dynamic in one run).
- **Acceptance:** On fallback, the resulting scene should still convey the message, albeit less dynamically. The user should ideally be informed post-generation that "Scene X used fallback visuals due to an engine issue," perhaps in a log or UI, but the video itself will not advertise this fact.
- **Voiceover Generation Failure:** If using TTS for narration and the TTS service fails or yields unusable output:
- If possible, try a backup TTS provider or voice (e.g., if primary voice API fails, try an alternate API or a local TTS engine).
- If no TTS works, fallback to a _text-only video_. This means we rely on captions as the primary communication: ensure captions (or large title text) are on-screen for each scene to convey the message. Optionally, play the background music a bit louder since no voice is present (to keep video from being completely silent aside from music).
- The system should still create the video with timing based on an estimated reading time of the captions (e.g., how long would it display each caption if voice is absent - could use a rule like 3 seconds per sentence or so).
- This ensures the user gets a video that, while missing voiceover, at least has text. Many social videos are actually consumed muted with captions, so it's still useful.
- **Audio Alignment Issues:** If the forced alignment for captions fails (maybe due to heavy noise or accent in voice such that transcript words don't align well):
- Fallback to a simpler method: e.g., split captions by sentence and assume even timing over the scene. Not as precise, but better than no captions or misaligned ones.
- If voiceover had issues with timing (like a segment went longer than scene), as a last resort we can adjust scene length slightly to accommodate (if it doesn't break overall length). Or fade out the voice if absolutely necessary (not ideal, but an option).
- Essentially, ensure the mismatch is resolved by either adjusting timing or truncating gracefully (fade out).
- **Background Music Failure:** If a music track fails to load or is unavailable:
- Simply proceed without music, or use a default royalty-free track as a replacement.
- Absence of music won't stop the video; the video might just be voice (and ambient silent gaps) - still usable. However, this is a rare/simple failure (maybe if file path was wrong).
- **Content Moderation / Safety:** If the AI engines refuse to generate certain content (e.g., because prompt has a banned word or the concept is sensitive):
- The Engine Manager can automatically attempt to rephrase the prompt by removing or substituting the flagged terms (if known). For instance, if "shoot" was in prompt meaning film, replace with "film". This rephrase attempt could be automated up to 1-2 tries.
- If still rejected, fallback to static as per engine failure above.
- Additionally, if a generated clip contains something unsafe (detected via a quick scan or human QA in loop), treat that as a failure and do not include it. Use fallback or regenerate with different parameters (e.g., add "family-friendly" to prompt or so).
- The system's bias should err on the side of safety: better to produce a boring scene than one that could cause a brand issue.
- **Timeline Overrun:** If for some reason the assembled video would exceed 60 seconds (maybe TTS spoke slower than expected or user added too many images):
- The system should trim or fast-forward rather than fail. For instance, slightly increase playback speed of AI clips or voice (like 1.1x speed, which might be unnoticeable if small) to fit the budget, or cut some non-critical parts (maybe drop a redundant scene if at risk of going over).
- This is a fallback approach to length enforcement. Ideally, planning prevents this, but fallback ensures we don't output a 70s video when spec says 60s max.
- If trimming, do it in a sensible way (e.g., cut an outro music early with a faster fade, etc.).
- **Partial Output / Crash Handling:** If at any stage an unexpected error occurs (say a bug causes an exception mid-way, or memory runs out during rendering):
- The system should catch top-level exceptions in the generation process and attempt to output _something_. Perhaps re-attempt rendering in a simpler mode:
  - For example, if the fancy composition failed (maybe due to a complex overlay), try a simpler composition (no transitions or simpler text rendering) as a backup path.
  - Or, as a last resort, output a video that is just a slideshow of the product images with captions. That would be akin to falling back all the way to V1 functionality. This ensures the user isn't left with nothing. It might not be pretty, but it's still a video conveying the content.
- Also, ensure temporary files are cleaned and no partial corrupt file is given. If only a partial video was rendered before crash, don't give that - better to have a simpler full video.
- **Engine Quality Degradation:** Sometimes engines don't fully fail but produce poor quality (e.g., very jittery video or irrelevant content):
- If an automated way to judge quality is present (not easy, but maybe extreme cases can be caught, like if the video is mostly blank or very low resolution), then treat it similar to failure and fallback or regenerate.
- Another strategy: If available, use multiple engines for the same scene and choose the best result (this is redundancy rather than fallback). Could be time-consuming, so maybe only do that for important scenes or in a high-quality mode.
- If the quality issue is slight, just proceed (since it's subjective); if severe (e.g., engine glitched and video is all noise), definitely swap it out with fallback.
- **Logging and User Notification:** For each fallback event, log it (for debugging and improving future versions). Optionally, if user-facing, we might notify the user that "We had to simplify scene 3 due to technical issues." But likely this spec is more about internal handling. The key is the user should _not_ have to intervene; the system handles it.

### Ensuring a Usable Output Always

By following the above rules, the system will always deliver a final MP4 video that is at least minimally acceptable: - If everything works: a high-quality, cinematic video as envisioned. - If some things fail: a simpler video (perhaps more static or text-heavy) but still conveying the message.

There should never be a case where the user gets an error like "Sorry, your video could not be generated." That is explicitly what we avoid by design[\[16\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,just%20error%20messages). Instead, the user gets a result in all cases. In the worst case scenario, that result might be equivalent to a V1-style slideshow with captions - not ideal, but it fulfills basic requirements and the user can use it or at least have something while we fix issues.

The system is effectively building resilience by having layers of fallback: - Engine fallback (AI to AI, then AI to static). - Audio fallback (TTS to alt TTS, then to none with text). - Timing fallback (adjust speeds or cut content if needed). - Rendering fallback (complex to simple pipeline if needed).

Additionally, isolation of components helps reliability: e.g., if one scene's generation fails, it doesn't derail other scenes - they each are separate, and timeline can still stitch what's available.

**Example of Graceful Degradation in action:** Suppose a user requests a video with two scenes: a product shot and an outdoor usage scene, with voiceover. If the internet is down when generating scene2, Pytoon V2 might still output: Scene1 (product image with caption and voice) as planned, Scene2 (instead of the AI video, maybe it shows the product image again with a different background color or slight movement, with the second caption and voice continuing). The transitions and captions happen as if nothing went wrong, and the video finishes normally. The user sees a somewhat repetitive visual but at least the voice and text deliver the message. They do not see a blank screen or an abrupt stop. Later, when engines are back, the user could retry for a better scene2, but the point is they weren't left empty-handed for that iteration.

Finally, a continuous improvement angle: each failure mode encountered should feed back into improving the system's rules. For example, if a particular word often triggers moderation, update the prompt scrubber to handle it in future. Or if a certain engine has a high failure rate at certain times, perhaps schedule around it or default to another engine at those times. Over time, the fallback would be needed less, but it's always there as a safety net.

By rigorously defining these reliability behaviors, Pytoon V2 aims to be robust in production settings. Marketing teams and creators can trust that _no matter what hiccups occur behind the scenes, they will get a video output for their input_, making the system dependable for everyday use[\[17\]](https://qodex.ai/blog/openai-sora-api#:~:text=3,to%20simpler%20functionality%20when%20needed). This reliability, combined with the enhanced capabilities, makes V2 a significant professional upgrade over V1.

[\[1\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=The%20AI%20video%20generation%20market,hard%20limits%20with%20existing%20solutions) [\[2\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=consistency%20Waiting%203%2B%20min%20for,Camera%20keyframes%20%26%20seed%20locking) [\[3\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=The%20Cinematic%20Triangle%20Framework) [\[12\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=Successful%20Seedance%20prompting%20follows%3A%20Cinematic,Scene%20Graph%20%E2%86%92%20Camera%20Tokens) [\[13\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=,camera%20tokens%20for%20deterministic%20cinematography) [\[22\]](https://superduperai.co/hi/blog/seedance-ai#:~:text=L20%20No%20multi,Camera%20keyframes%20%26%20seed%20locking) SuperDuperAI - AI Video Generation Platform

<https://superduperai.co/hi/blog/seedance-ai>

[\[4\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=You%20can%20generate%20directly%20for,and%20background%20consistency%20improved%20significantly) [\[5\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=turnaround%20stuff,publish%20all%20in%20one%20flow) [\[6\]](https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/#:~:text=Google%20updated%20Veo%203,fit%20Instagram%20Reels%20or%20TikTok) Google Veo now generates native 9:16 vertical videos for social platforms : r/socialmedia

<https://www.reddit.com/r/socialmedia/comments/1qea3sg/google_veo_now_generates_native_916_vertical/>

[\[7\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=%2A%20Platform%20specialization%20beats%20one,first%20campaigns) [\[8\]](https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai#:~:text=While%20Runway%20and%20Pika%20compete,backed%20outputs%20with%20naturalistic%20motion) Runway vs Pika vs Luma AI - A Complete Guide for Marketing Leaders in 2026

<https://genesysgrowth.com/blog/runway-vs-pika-vs-luma-ai>

[\[9\]](https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA#:~:text=Using%20Audio%20Ducking%20to%20Balance,speech%20can%20be%20clearly) Using Audio Ducking to Balance Voice-overs and Background Music

<https://www.cyberlink.com/learning/powerdirector-video-editing-software/824/using-audio-ducking-to-balance-voice-overs-and-background-music?srsltid=AfmBOopbabJqU-nx2-KmjrcHgJvJQFjQTPZYM2lC1AlgalPknrGuyCYA>

[\[10\]](https://arxiv.org/pdf/2512.18318#:~:text=,ment%2C%20achieving%20an) \[PDF\] Asynchronous Pipeline Parallelism for Real-Time Multilingual Lip ...

<https://arxiv.org/pdf/2512.18318>

[\[11\]](https://key-g.com/pt/blog/veo-3-the-future-of-video-generation-now-with-visual-instructions/#:~:text=Adopt%20a%20modular%20prompt%20schema,and%20precise%20updates%20across%20frames) [\[14\]](https://key-g.com/pt/blog/veo-3-the-future-of-video-generation-now-with-visual-instructions/#:~:text=To%20support%20%D0%B8%D0%B3%D1%80%D1%8B%20scenarios%2C%20define,clear%20prompts%20rather%20than%20guesswork) Veo 3 Future of Video Generation with Visual Instructions

<https://key-g.com/pt/blog/veo-3-the-future-of-video-generation-now-with-visual-instructions/>

[\[15\]](https://glama.ai/mcp/servers/@naki0227/auto-cm-director#:~:text=The%20Hybrid%20Architecture) [\[18\]](https://glama.ai/mcp/servers/@naki0227/auto-cm-director#:~:text=Unlike%20traditional%20%22Text,uses%20a%20Hybrid%20Template%20Engine) auto-cm-director by naki0227 | Glama

<https://glama.ai/mcp/servers/@naki0227/auto-cm-director>

[\[16\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,just%20error%20messages) [\[19\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=,capable%20with%20progressive%20enhancement) [\[20\]](https://paddo.dev/blog/when-not-to-use-ai/#:~:text=attempts%20,just%20error%20messages) When Not to Use AI: Two Approaches to Building AI-Powered Products

<https://paddo.dev/blog/when-not-to-use-ai/>

[\[17\]](https://qodex.ai/blog/openai-sora-api#:~:text=3,to%20simpler%20functionality%20when%20needed) How to Use OpenAI's Sora API: A Comprehensive Guide | Qodex.ai

<https://qodex.ai/blog/openai-sora-api>

[\[21\]](https://helpx.adobe.com/premiere/desktop/add-audio-effects/adjust-volume-and-levels/automatically-duck-audio.html#:~:text=Automatically%20duck%20audio%20,dialogue%20or%20voiceover%20is) Automatically duck audio - Adobe Help Center

<https://helpx.adobe.com/premiere/desktop/add-audio-effects/adjust-volume-and-levels/automatically-duck-audio.html>